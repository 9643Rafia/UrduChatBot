{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc02a5b",
   "metadata": {},
   "source": [
    "\n",
    "# Urdu Chatbot From Scratch (No Pretrained Models) — Colab Notebook\n",
    "\n",
    "This notebook trains a **small Transformer encoder–decoder built from scratch** (pure PyTorch) for an Urdu chatbot.  \n",
    "We synthesize dialogue pairs from your dataset by pairing consecutive sentences: `(utterance_t → utterance_{t+1})`.\n",
    "\n",
    "**What you get:**\n",
    "- Urdu text normalization/tokenization (no external tokenizers)\n",
    "- Vocabulary built **only** from your dataset\n",
    "- Transformer (multi-head attention, positional enc., masking) **from scratch**\n",
    "- Training with teacher forcing\n",
    "- Evaluation: BLEU, ROUGE-L, chrF, Perplexity\n",
    "- Greedy & beam search decoding\n",
    "- Checkpoint saved by best **validation BLEU**\n",
    "\n",
    "> ⚠️ **No pretrained models or embeddings are used.** Everything is trained from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Colab, you likely already have torch & numpy.\n",
    "# Optional: upgrade pip packages if you get version warnings.\n",
    "# !pip -q install torch pandas numpy\n",
    "\n",
    "import os, math, re, unicodedata, random, json\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d8bc9",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load your dataset (`.tsv`)\n",
    "Choose **one** of the following:\n",
    "- **A. Upload from your computer** (recommended for this assignment)\n",
    "- **B. Load from Google Drive** (if you placed the file there)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1de8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (A) Upload from your computer\n",
    "# After uploading, set DATA_PATH to the exact file name displayed (e.g., 'final_main_dataset.tsv')\n",
    "try:\n",
    "    from google.colab import files  # type: ignore\n",
    "    print(\"Use the dialog to upload your TSV (e.g., final_main_dataset.tsv)\")\n",
    "    uploaded = files.upload()\n",
    "    DATA_PATH = list(uploaded.keys())[0] if uploaded else \"final_main_dataset.tsv\"\n",
    "except Exception as e:\n",
    "    print(\"Not on Colab or files.upload not available. Set DATA_PATH manually if needed.\")\n",
    "    DATA_PATH = \"final_main_dataset.tsv\"\n",
    "\n",
    "print(\"DATA_PATH:\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4234892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (B) Load from Google Drive (optional)\n",
    "# from google.colab import drive  # type: ignore\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_PATH = '/content/drive/MyDrive/path/to/final_main_dataset.tsv'\n",
    "# print(\"DATA_PATH:\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f5680",
   "metadata": {},
   "source": [
    "\n",
    "### Inspect the TSV\n",
    "We expect a **`sentence`** column (as in Common Voice Urdu). If your column name differs, update `TEXT_COL` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(DATA_PATH, sep=\"\\t\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick the text column (update if needed)\n",
    "TEXT_COL = 'sentence' if 'sentence' in df.columns else df.columns[0]\n",
    "texts = df[TEXT_COL].astype(str).tolist()\n",
    "print(\"Using text column:\", TEXT_COL, \" | Total lines:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75b7a4",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Urdu normalization & tokenization\n",
    "Simple, assignment-friendly rules:\n",
    "- remove diacritics\n",
    "- unify select Arabic/Urdu variants\n",
    "- whitespace/punctuation token split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DIACRITICS = [\n",
    "    \"\\u064B\",\"\\u064C\",\"\\u064D\",\"\\u064E\",\"\\u064F\",\"\\u0650\",\"\\u0651\",\"\\u0652\",\n",
    "    \"\\u0653\",\"\\u0654\",\"\\u0655\",\"\\u0670\"\n",
    "]\n",
    "import re\n",
    "DIACRITICS_RE = re.compile(\"|\".join(map(re.escape, DIACRITICS)))\n",
    "PUNCT = r\"([\\,\\.\\!\\?\\؛\\:\\(\\)\\[\\]\\{\\}«»\\\"'،۔:؛؟])\"\n",
    "\n",
    "def normalize_urdu(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    t = unicodedata.normalize(\"NFC\", t)\n",
    "    t = DIACRITICS_RE.sub(\"\", t)\n",
    "    t = t.replace(\"آ\", \"ا\").replace(\"أ\", \"ا\").replace(\"إ\", \"ا\")\n",
    "    t = t.replace(\"ي\", \"ی\").replace(\"ى\", \"ی\").replace(\"ك\", \"ک\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "def tokenize_urdu(text: str) -> List[str]:\n",
    "    t = normalize_urdu(text)\n",
    "    t = re.sub(PUNCT, r\" \\1 \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t.split(\" \") if t else []\n",
    "\n",
    "# Quick sanity check:\n",
    "for s in texts[:3]:\n",
    "    print(s, \" -> \", tokenize_urdu(s)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed40239",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Build synthetic dialogue pairs\n",
    "We create `(input → response)` by pairing consecutive sentences. Empty/very short items are filtered out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 80  # tokens (incl. SOS/EOS later)\n",
    "\n",
    "raw_pairs = []\n",
    "for i in range(len(texts)-1):\n",
    "    a = tokenize_urdu(texts[i])[:MAX_LEN-2]\n",
    "    b = tokenize_urdu(texts[i+1])[:MAX_LEN-2]\n",
    "    if a and b:\n",
    "        raw_pairs.append((a,b))\n",
    "\n",
    "len(raw_pairs), raw_pairs[0][:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fedaf8",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Build vocabulary (from scratch)\n",
    "Special tokens: `<pad>`, `<s>`, `</s>`, `<unk>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e27fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PAD, SOS, EOS, UNK = \"<pad>\", \"<s>\", \"</s>\", \"<unk>\"\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens, min_freq=2, max_size=30000):\n",
    "        freq = Counter(tok for sent in tokens for tok in sent)\n",
    "        self.itos = [PAD, SOS, EOS, UNK]\n",
    "        for tok, c in freq.most_common():\n",
    "            if c < min_freq: break\n",
    "            if tok in self.itos: continue\n",
    "            if len(self.itos) >= max_size: break\n",
    "            self.itos.append(tok)\n",
    "        self.stoi = {s:i for i,s in enumerate(self.itos)}\n",
    "    def encode(self, toks, add_sos_eos=True):\n",
    "        ids = [self.stoi.get(t, self.stoi[UNK]) for t in toks]\n",
    "        return [self.stoi[SOS]] + ids + [self.stoi[EOS]] if add_sos_eos else ids\n",
    "    def decode(self, ids):\n",
    "        return [self.itos[i] if i < len(self.itos) else UNK for i in ids]\n",
    "    def __len__(self): return len(self.itos)\n",
    "\n",
    "src_tokens = [a for a,_ in raw_pairs]\n",
    "tgt_tokens = [b for _,b in raw_pairs]\n",
    "src_vocab = Vocab(src_tokens, min_freq=2, max_size=30000)\n",
    "tgt_vocab = Vocab(tgt_tokens, min_freq=2, max_size=30000)\n",
    "len(src_vocab), len(tgt_vocab), src_vocab.itos[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5f8a0",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Dataset & DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab, max_len=80):\n",
    "        self.pairs = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        a,b = self.pairs[idx]\n",
    "        s = torch.tensor(self.src_vocab.encode(a, add_sos_eos=True), dtype=torch.long)\n",
    "        t = torch.tensor(self.tgt_vocab.encode(b, add_sos_eos=True), dtype=torch.long)\n",
    "        return s,t\n",
    "\n",
    "def pad_batch(samples, pad_idx):\n",
    "    srcs, tgts = zip(*samples)\n",
    "    sl = max(len(s) for s in srcs)\n",
    "    tl = max(len(t) for t in tgts)\n",
    "    b = len(samples)\n",
    "    S = torch.full((b,sl), pad_idx, dtype=torch.long)\n",
    "    T = torch.full((b,tl), pad_idx, dtype=torch.long)\n",
    "    for i,(s,t) in enumerate(samples):\n",
    "        S[i,:len(s)] = s\n",
    "        T[i,:len(t)] = t\n",
    "    return S,T\n",
    "\n",
    "full_ds = PairDataset(raw_pairs, src_vocab, tgt_vocab, max_len=MAX_LEN)\n",
    "N = len(full_ds)\n",
    "val_frac, test_frac = 0.10, 0.10\n",
    "n_test = int(N*test_frac)\n",
    "n_val  = int(N*val_frac)\n",
    "n_train = N - n_val - n_test\n",
    "train_ds, val_ds, test_ds = random_split(full_ds, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "collate = lambda batch: pad_batch(batch, pad_idx=tgt_vocab.stoi[PAD])\n",
    "train_it = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "val_it   = DataLoader(val_ds, batch_size=64, shuffle=False, collate_fn=collate)\n",
    "test_it  = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb0776",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Transformer (from scratch)\n",
    "Encoder–decoder with multi-head attention, positional encodings, masking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    mask = torch.triu(torch.ones(sz, sz, dtype=torch.bool), diagonal=1)\n",
    "    return mask.unsqueeze(0)\n",
    "\n",
    "def make_pad_mask(seq: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
    "    return (seq == pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.h = heads; self.d_k = d_model // heads\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.o = nn.Linear(d_model, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, Tq, d = q.shape; Tk = k.shape[1]\n",
    "        q = self.q(q).view(B, Tq, self.h, self.d_k).transpose(1,2)\n",
    "        k = self.k(k).view(B, Tk, self.h, self.d_k).transpose(1,2)\n",
    "        v = self.v(v).view(B, Tk, self.h, self.d_k).transpose(1,2)\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None: scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        ctx = (self.drop(attn) @ v).transpose(1,2).contiguous().view(B, Tq, self.h*self.d_k)\n",
    "        return self.o(ctx)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.ReLU(inplace=True), nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.n1 = nn.LayerNorm(d_model); self.n2 = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.n1(x + self.drop(self.sa(x,x,x, mask=src_mask)))\n",
    "        x = self.n2(x + self.drop(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ca = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.n1 = nn.LayerNorm(d_model); self.n2 = nn.LayerNorm(d_model); self.n3 = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x, mem, tgt_mask, tgt_pad_mask, src_mask):\n",
    "        x = self.n1(x + self.drop(self.sa(x,x,x, mask=tgt_mask | tgt_pad_mask)))\n",
    "        x = self.n2(x + self.drop(self.ca(x,mem,mem, mask=src_mask)))\n",
    "        x = self.n3(x + self.drop(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model, layers, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, dropout) for _ in range(layers)])\n",
    "    def forward(self, src, src_mask):\n",
    "        x = self.pe(self.emb(src) * math.sqrt(self.emb.embedding_dim))\n",
    "        for l in self.layers: x = l(x, src_mask)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model, layers, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff, dropout) for _ in range(layers)])\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    def forward(self, tgt, mem, tgt_mask, tgt_pad_mask, src_mask):\n",
    "        x = self.pe(self.emb(tgt) * math.sqrt(self.emb.embedding_dim))\n",
    "        for l in self.layers: x = l(x, mem, tgt_mask, tgt_pad_mask, src_mask)\n",
    "        return self.proj(x)\n",
    "\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, enc_layers=2, dec_layers=2, heads=2, d_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, enc_layers, heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, dec_layers, heads, d_ff, dropout)\n",
    "        self.pad_idx = 0\n",
    "    def forward(self, src, tgt_inp):\n",
    "        src_mask = make_pad_mask(src, self.pad_idx)\n",
    "        tgt_pad = make_pad_mask(tgt_inp, self.pad_idx)\n",
    "        causal = subsequent_mask(tgt_inp.size(1)).to(tgt_inp.device)\n",
    "        mem = self.encoder(src, src_mask)\n",
    "        out = self.decoder(tgt_inp, mem, causal, tgt_pad, src_mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8378a",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Metrics\n",
    "BLEU, ROUGE-L, chrF, Perplexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ngrams(seq, n): return [tuple(seq[i:i+n]) for i in range(len(seq)-n+1)]\n",
    "\n",
    "def bleu_score(ref: List[str], hyp: List[str], max_n=4, smooth=1e-9):\n",
    "    weights = [1.0/max_n]*max_n\n",
    "    hyp_len, ref_len = len(hyp), len(ref)\n",
    "    if hyp_len == 0: return 0.0\n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        h = Counter(ngrams(hyp, n)); r = Counter(ngrams(ref, n))\n",
    "        overlap = sum(min(c, r[g]) for g,c in h.items())\n",
    "        total = max(sum(h.values()), 1)\n",
    "        precisions.append((overlap + smooth) / (total + smooth))\n",
    "    geo = math.exp(sum(w*math.log(p) for w,p in zip(weights, precisions)))\n",
    "    bp = 1.0 if hyp_len > ref_len else math.exp(1 - ref_len/max(hyp_len,1))\n",
    "    return bp * geo\n",
    "\n",
    "def rouge_l(ref: List[str], hyp: List[str]):\n",
    "    m, n = len(ref), len(hyp)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            dp[i+1][j+1] = dp[i][j]+1 if ref[i]==hyp[j] else max(dp[i][j+1], dp[i+1][j])\n",
    "    lcs = dp[m][n]\n",
    "    if lcs == 0: return 0.0\n",
    "    prec, rec = lcs/max(n,1), lcs/max(m,1)\n",
    "    return 0.0 if (prec+rec)==0 else (2*prec*rec)/(prec+rec)\n",
    "\n",
    "def chrf(ref: str, hyp: str, n=6, beta=2.0):\n",
    "    def counts(s):\n",
    "        s = s.replace(\" \", \"\")\n",
    "        out = {}\n",
    "        for k in range(1, n+1):\n",
    "            grams = ngrams(list(s), k); out[k] = Counter(grams)\n",
    "        return out\n",
    "    R, H = counts(ref), counts(hyp)\n",
    "    Fs = []\n",
    "    for k in range(1, n+1):\n",
    "        overlap = sum((R[k] & H[k]).values())\n",
    "        r_tot, h_tot = sum(R[k].values()), sum(H[k].values())\n",
    "        if r_tot == 0 or h_tot == 0: Fs.append(0.0); continue\n",
    "        prec, rec = overlap / h_tot, overlap / r_tot\n",
    "        Fs.append(0.0 if (prec+rec)==0 else (1+beta*beta)*prec*rec/(beta*beta*prec+rec))\n",
    "    return float(np.mean(Fs))\n",
    "\n",
    "def perplexity(loss): return float(math.exp(min(20, loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473cfaf",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Training / Evaluation / Decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, data_iter, tgt_vocab, device):\n",
    "    model.eval()\n",
    "    pad = tgt_vocab.stoi[PAD]\n",
    "    ce = nn.CrossEntropyLoss(ignore_index=pad, reduction=\"sum\")\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    bleu_list, rouge_list, chrf_list = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_iter:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            inp, gold = tgt[:, :-1], tgt[:, 1:]\n",
    "            logits = model(src, inp)\n",
    "            B,T,V = logits.shape\n",
    "            loss = ce(logits.view(B*T, V), gold.reshape(B*T))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += (gold != pad).sum().item()\n",
    "            hyps = greedy_decode(model, src, tgt_vocab, max_len=inp.size(1)+20, device=device)\n",
    "            for i in range(src.size(0)):\n",
    "                ref_ids = gold[i].tolist()\n",
    "                ref_toks = [t for t in tgt_vocab.decode(ref_ids) if t not in (PAD,SOS,EOS)]\n",
    "                hyp_toks = hyps[i]\n",
    "                bleu_list.append(bleu_score(ref_toks, hyp_toks))\n",
    "                rouge_list.append(rouge_l(ref_toks, hyp_toks))\n",
    "                chrf_list.append(chrf(\"\".join(ref_toks), \"\".join(hyp_toks)))\n",
    "    ppl = perplexity(total_loss / max(total_tokens,1))\n",
    "    return {\"BLEU\": float(np.mean(bleu_list)) if bleu_list else 0.0,\n",
    "            \"ROUGE_L\": float(np.mean(rouge_list)) if rouge_list else 0.0,\n",
    "            \"chrF\": float(np.mean(chrf_list)) if chrf_list else 0.0,\n",
    "            \"Perplexity\": ppl}\n",
    "\n",
    "def train_loop(model, train_it, val_it, tgt_vocab, epochs=8, lr=3e-4, save_dir=\"./runs_urdu_bot\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    pad = tgt_vocab.stoi[PAD]\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    best_bleu = -1.0\n",
    "    ckpt = os.path.join(save_dir, \"best_bleu.pt\")\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running, steps = 0.0, 0\n",
    "        for src, tgt in train_it:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            inp, gold = tgt[:, :-1], tgt[:, 1:]\n",
    "            logits = model(src, inp)\n",
    "            B,T,V = logits.shape\n",
    "            loss = criterion(logits.view(B*T, V), gold.reshape(B*T))\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            running += loss.item(); steps += 1\n",
    "\n",
    "        val_metrics = evaluate(model, val_it, tgt_vocab, device)\n",
    "        print(f\"[Epoch {ep}] train_loss={running/max(steps,1):.4f} | \"\n",
    "              f\"val_BLEU={val_metrics['BLEU']:.4f} ROUGE_L={val_metrics['ROUGE_L']:.4f} \"\n",
    "              f\"chrF={val_metrics['chrF']:.4f} PPL={val_metrics['Perplexity']:.2f}\")\n",
    "\n",
    "        if val_metrics[\"BLEU\"] > best_bleu:\n",
    "            best_bleu = val_metrics[\"BLEU\"]\n",
    "            torch.save({\"model_state\": model.state_dict()}, ckpt)\n",
    "            print(\"  -> Saved new best checkpoint:\", ckpt)\n",
    "\n",
    "def greedy_decode(model, src, tgt_vocab, max_len=60, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    pad = tgt_vocab.stoi[PAD]; sos = tgt_vocab.stoi[SOS]; eos = tgt_vocab.stoi[EOS]\n",
    "    with torch.no_grad():\n",
    "        src_mask = make_pad_mask(src, pad)\n",
    "        mem = model.encoder(src, src_mask)\n",
    "        B = src.size(0)\n",
    "        ys = torch.full((B,1), sos, dtype=torch.long, device=device)\n",
    "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "        outs = [[] for _ in range(B)]\n",
    "        for _ in range(max_len):\n",
    "            tgt_pad = make_pad_mask(ys, pad)\n",
    "            causal = subsequent_mask(ys.size(1)).to(device)\n",
    "            logits = model.decoder(ys, mem, causal, tgt_pad, src_mask)\n",
    "            nxt = logits[:,-1,:].argmax(-1)\n",
    "            ys = torch.cat([ys, nxt.unsqueeze(1)], dim=1)\n",
    "            for i,t in enumerate(nxt.tolist()):\n",
    "                if not finished[i]:\n",
    "                    if t == eos: finished[i] = True\n",
    "                    else: outs[i].append(tgt_vocab.itos[t] if t < len(tgt_vocab.itos) else UNK)\n",
    "            if finished.all(): break\n",
    "        return outs\n",
    "\n",
    "def beam_search_decode(model, src, tgt_vocab, beam=4, max_len=60, device=\"cpu\"):\n",
    "    pad = tgt_vocab.stoi[PAD]; sos = tgt_vocab.stoi[SOS]; eos = tgt_vocab.stoi[EOS]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_mask = make_pad_mask(src, pad)\n",
    "        mem = model.encoder(src, src_mask)\n",
    "        B = src.size(0)\n",
    "        results = [[] for _ in range(B)]\n",
    "        for b in range(B):\n",
    "            beams = [(0.0, [sos])]\n",
    "            for _ in range(max_len):\n",
    "                new_beams = []\n",
    "                for lp, seq in beams:\n",
    "                    if seq[-1] == eos:\n",
    "                        new_beams.append((lp, seq)); continue\n",
    "                    ys = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "                    tgt_pad = make_pad_mask(ys, pad)\n",
    "                    causal = subsequent_mask(ys.size(1)).to(device)\n",
    "                    logits = model.decoder(ys, mem[b:b+1], causal, tgt_pad, src_mask[b:b+1])\n",
    "                    logp = torch.log_softmax(logits[:,-1,:], dim=-1).squeeze(0)\n",
    "                    topk = torch.topk(logp, beam)\n",
    "                    for kprob, idx in zip(topk.values.tolist(), topk.indices.tolist()):\n",
    "                        new_beams.append((lp + kprob, seq + [idx]))\n",
    "                beams = sorted(new_beams, key=lambda x:x[0], reverse=True)[:beam]\n",
    "                if all(s[-1]==eos for _,s in beams): break\n",
    "            best = max(beams, key=lambda x:x[0])[1]\n",
    "            toks = [t for t in best[1:] if t != eos]\n",
    "            results[b] = [tgt_vocab.itos[t] if t < len(tgt_vocab.itos) else UNK for t in toks]\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc8ac0",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Train\n",
    "You can tweak hyperparameters below for speed vs. quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b81bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "D_MODEL = 256\n",
    "HEADS = 2\n",
    "ENC_LAYERS = 2\n",
    "DEC_LAYERS = 2\n",
    "D_FF = 1024\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 8      # Increase for better results if you have time/GPU\n",
    "BATCH = 64\n",
    "LR = 3e-4\n",
    "SAVE_DIR = \"./urdu_runs\"\n",
    "\n",
    "model = TransformerSeq2Seq(\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    d_model=D_MODEL, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,\n",
    "    heads=HEADS, d_ff=D_FF, dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "train_loop(model, train_it, val_it, tgt_vocab, epochs=EPOCHS, lr=LR, save_dir=SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24219bda",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Evaluate on Test Set\n",
    "(Loads best checkpoint by validation BLEU if present.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ckpt = os.path.join(SAVE_DIR, \"best_bleu.pt\")\n",
    "if os.path.exists(ckpt):\n",
    "    state = torch.load(ckpt, map_location=device)\n",
    "    model.load_state_dict(state[\"model_state\"])\n",
    "    print(\"Loaded best checkpoint.\")\n",
    "\n",
    "test_metrics = evaluate(model, test_it, tgt_vocab, device)\n",
    "print(\"[TEST] BLEU={:.4f} ROUGE_L={:.4f} chrF={:.4f} PPL={:.2f}\".format(\n",
    "    test_metrics[\"BLEU\"], test_metrics[\"ROUGE_L\"], test_metrics[\"chrF\"], test_metrics[\"Perplexity\"]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63e5ae",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Try chatting (Greedy / Beam)\n",
    "Enter an Urdu sentence; the model will generate the next-utterance reply.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de056363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_single(sentence, src_vocab, max_len=80):\n",
    "    toks = tokenize_urdu(sentence)[:max_len-2]\n",
    "    ids = torch.tensor([src_vocab.encode(toks, add_sos_eos=True)], dtype=torch.long).to(device)\n",
    "    return ids\n",
    "\n",
    "def reply(text, beam=0):\n",
    "    src = encode_single(text, src_vocab)\n",
    "    if beam and beam > 0:\n",
    "        outs = beam_search_decode(model, src, tgt_vocab, beam=beam, device=device)\n",
    "    else:\n",
    "        outs = greedy_decode(model, src, tgt_vocab, device=device)\n",
    "    return \" \".join(outs[0])\n",
    "\n",
    "print(\"Example:\")\n",
    "print(\"You:\", \"آپ کیسے ہیں؟\")\n",
    "print(\"Bot:\", reply(\"آپ کیسے ہیں؟\", beam=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ec8dc",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Save vocab (optional)\n",
    "To reuse the same vocabulary later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb40cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle, pathlib\n",
    "pathlib.Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "with open(os.path.join(SAVE_DIR, \"src_vocab.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"itos\": src_vocab.itos, \"stoi\": src_vocab.stoi}, f)\n",
    "with open(os.path.join(SAVE_DIR, \"tgt_vocab.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"itos\": tgt_vocab.itos, \"stoi\": tgt_vocab.stoi}, f)\n",
    "print(\"Saved:\", os.path.join(SAVE_DIR, \"src_vocab.pkl\"), \"and tgt_vocab.pkl\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
